{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "こちらはgithubで管理できるように出力結果を全て削除したものです。出力結果も確認したい場合は以下のリンクのnotebookをご覧ください。\n",
        "https://colab.research.google.com/drive/1TkIpSH1yFp-OJEbbqrMMscnXVjFrN7JC?usp=sharing"
      ],
      "metadata": {
        "id": "rWpqH08sq-iS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bla6WHyQStsO"
      },
      "source": [
        "### 演習環境の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM50WAI7GXwC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install google-colab-selenium\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2PStE0uqM03"
      },
      "outputs": [],
      "source": [
        "# 演習用のコンテンツを取得\n",
        "!git clone https://github.com/shinybell/rag-experiment.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd rag-experiment && git pull"
      ],
      "metadata": {
        "id": "uoYgsZXa1_FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXo_kFASXlvp"
      },
      "outputs": [],
      "source": [
        "# HuggingFace Login\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ_NUIftXwLc"
      },
      "outputs": [],
      "source": [
        "# CUDAが利用可能ならGPUを、それ以外ならCPUをデバイスとして設定\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eTgV8XBPA90"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tV9mO8oXoaM"
      },
      "outputs": [],
      "source": [
        "# モデル(Gemma2)の読み込み\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"google/gemma-2-2b-jpn-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piTdVxTfGcc_"
      },
      "source": [
        "# 1. ベースラインモデル評価\n",
        "**まずはベースモデルがどの程度知識を持っているか確かめる**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_output(query):\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": query},\n",
        "  ]\n",
        "  input_ids = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(model.device)\n",
        "\n",
        "  terminators = [\n",
        "      tokenizer.eos_token_id,\n",
        "      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "  outputs = model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=256,\n",
        "      eos_token_id=terminators,\n",
        "      do_sample=False,\n",
        "      # temperature=0.6, # If do_sample=True\n",
        "      # top_p=0.9,  # If do_sample=True\n",
        "  )\n",
        "\n",
        "  response = outputs[0][input_ids.shape[-1]:]\n",
        "  return tokenizer.decode(response, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "huDNYbXW3lOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "question_df = pd.read_csv(\"/content/rag-experiment/data/2025questions.csv\")\n",
        "question_df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "yfHciLNVzUHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responses = []\n",
        "for index, row in question_df.iterrows():\n",
        "    question = row['Question']\n",
        "    response = generate_output(question)\n",
        "    responses.append(response)"
      ],
      "metadata": {
        "id": "XjrqcCKv1kau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# responsesをdataframeに追加\n",
        "result_df = question_df.copy()\n",
        "result_df['withoutRAG'] = responses\n",
        "result_df.head()"
      ],
      "metadata": {
        "id": "1gd3oU_D4SFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 数値的な評価も見てみます。RagasにはAnswer Accuracyという評価指標があります。今回はこちらを参考に実装した評価関数を利用して測っていきます。\n",
        "\n",
        "- 今回はgemmaでは性能が不安定だったので、OpenAIのgpt-4oで評価していきます。従って、scoreの実行はopenAI APIキーを所持している関心がある方のみで良いです。"
      ],
      "metadata": {
        "id": "fG9zI6_lAYsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai"
      ],
      "metadata": {
        "id": "Fncw05_I_IVl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 評価実装\n",
        "# gold_answer = \"「Inference Time Scaling」とは、推論時に計算量を増やしてモデルの性能を高める手法です。これはモデルのサイズを大きくする代わりに、難しい入力に対して多くの計算リソースを使うことで、より良い出力を得ようとするアプローチです。\"\n",
        "\n",
        "# from openai import OpenAI\n",
        "# from google.colab import userdata\n",
        "# client = OpenAI(api_key=userdata.get(\"OPENAI_API_KEY\"), max_retries=5, timeout=60)\n",
        "\n",
        "# def openai_generator(query):\n",
        "\n",
        "#         messages = [\n",
        "#                     {\n",
        "#                         \"role\": \"user\",\n",
        "#                         \"content\": query\n",
        "#                     }\n",
        "#                 ]\n",
        "\n",
        "#         response = client.chat.completions.create(\n",
        "#             model=\"gpt-4o-mini\",\n",
        "#             messages=messages\n",
        "#         )\n",
        "#         return response.choices[0].message.content\n",
        "\n",
        "# gemini\n",
        "\n",
        "# def evaluate_answer_accuracy(query, response, reference):\n",
        "\n",
        "#     template_accuracy1 = (\n",
        "#           \"Instruction: You are a world class state of the art assistant for rating \"\n",
        "#           \"a User Answer given a Question. The Question is completely answered by the Reference Answer.\\n\"\n",
        "#           \"Say 4, if User Answer is full contained and equivalent to Reference Answer\"\n",
        "#           \"in all terms, topics, numbers, metrics, dates and units.\\n\"\n",
        "#           \"Say 2, if User Answer is partially contained and almost equivalent to Reference Answer\"\n",
        "#           \"in all terms, topics, numbers, metrics, dates and units.\\n\"\n",
        "#           \"Say 0, if User Answer is not contained in Reference Answer or not accurate in all terms, topics,\"\n",
        "#           \"numbers, metrics, dates and units or the User Answer do not answer the question.\\n\"\n",
        "#           \"Do not explain or justify your rating. Your rating must be only 4, 2 or 0 according to the instructions above.\\n\"\n",
        "#           \"Even small discrepancies in meaning, terminology, directionality, or implication must result in a lower score. Only rate 4 if the User Answer is a complete and precise match to the Reference Answer in every aspect.\\n\"\n",
        "#           \"### Question: {query}\\n\"\n",
        "#           \"### {answer0}: {sentence_inference}\\n\"\n",
        "#           \"### {answer1}: {sentence_true}\\n\"\n",
        "#           \"The rating is:\\n\"\n",
        "#       )\n",
        "#     template_accuracy2 = (\n",
        "#           \"I will rate the User Answer in comparison to the Reference Answer for a given Question.\\n\"\n",
        "#           \"A rating of 4 indicates that the User Answer is entirely consistent with the Reference Answer, covering all aspects, topics, numbers, metrics, dates, and units.\\n\"\n",
        "#           \"A rating of 2 signifies that the User Answer is mostly aligned with the Reference Answer, with minor discrepancies in some areas.\\n\"\n",
        "#           \"A rating of 0 means that the User Answer is either inaccurate, incomplete, or unrelated to the Reference Answer, or it fails to address the Question.\\n\"\n",
        "#           \"I will provide the rating without any explanation or justification, adhering to the following scale: 0 (no match), 2 (partial match), 4 (exact match).\\n\"\n",
        "#           \"Even minor inconsistencies in meaning, terminology, emphasis, or factual detail should prevent a rating of 4. Only assign a 4 if the User Answer exactly and unambiguously matches the Reference Answer in every respect.\"\n",
        "#           \"Do not explain or justify my rating. My rating must be only 4, 2 or 0 only.\\n\\n\"\n",
        "#           \"Question: {query}\\n\\n\"\n",
        "#           \"{answer0}: {sentence_inference}\\n\\n\"\n",
        "#           \"{answer1}: {sentence_true}\\n\\n\"\n",
        "#           \"Rating: \"\n",
        "#       )\n",
        "\n",
        "#     score1 = openai_generator(\n",
        "#                 template_accuracy1.format(\n",
        "#                       query=query,\n",
        "#                       answer0=\"User Answer\",\n",
        "#                       answer1=\"Reference Answer\",\n",
        "#                       sentence_inference=response,\n",
        "#                       sentence_true=reference,\n",
        "#                     )\n",
        "#                 )\n",
        "#     try:\n",
        "#       score1 = int(score1)\n",
        "#     except:\n",
        "#       print(\"Failed\")\n",
        "#       score1 = 0\n",
        "\n",
        "#     score2 = openai_generator(\n",
        "#                 template_accuracy2.format(\n",
        "#                         query=query,\n",
        "#                         answer0=\"Reference Answer\",\n",
        "#                         answer1=\"User Answer\",\n",
        "#                         sentence_inference=reference,\n",
        "#                         sentence_true=response,\n",
        "#                     )\n",
        "#                   )\n",
        "\n",
        "#     try:\n",
        "#       score2 = int(score2)\n",
        "#     except:\n",
        "#       print(\"Failed\")\n",
        "#       score2 = 0\n",
        "\n",
        "\n",
        "#     return (score1 + score2) / 2"
      ],
      "metadata": {
        "id": "t89v938Y1o4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 評価\n",
        "# score = evaluate_answer_accuracy(question, response, gold_answer)\n",
        "# print(score)"
      ],
      "metadata": {
        "id": "CPLGyk7T5LaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSCNnRf9pJif"
      },
      "source": [
        "## 結果 (ベースモデル)\n",
        "\n",
        "「google/gemma-2-2b-jpn-it」は「Inference Time Scaling」について誤った知識を提示しました：\n",
        "* モデルは従来の「推論時間の短縮」という文脈でInference Time Scalingを解釈しており、これはLLM分野における最新の「Inference Time Scaling」概念（推論時計算資源の最適配分）とは異なる説明になります。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4R-hiKNGyJd"
      },
      "source": [
        "# 2. 文字起こしデータの活用"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 講義内容をソースとして活用 (RAG導入)\n",
        "\n",
        "モデルの回答の事実性を向上させるためにRetrieval Augmented Generation (RAG)技術を導入します：\n",
        "\n",
        "* **知識ソース**: LLM講座第4講における講師の発言内容\n",
        "* **目的**: モデルに「Inference Time Scaling」に関する正確な知識と文脈を提供し、事実に基づいた回答を促す\n",
        "\n",
        "**初期RAG実装（ベーシックアプローチ）**:\n",
        "* **ドキュメント処理**: 音声認識モデル(speech2text)で書き起こした生テキストをそのまま使用\n",
        "* **分割方法**: 「。」（句点）で区切られた文単位でテキストを分割\n",
        "* **検索手法**: シンプルな類似度ベースの検索でクエリに関連する文を抽出\n",
        "* **制約条件**: モデルの入力トークン制限に収まるよう関連文のみを選択"
      ],
      "metadata": {
        "id": "qcZCmKegHyrl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47GvcceyObAl"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "emb_model = SentenceTransformer(\"infly/inf-retriever-v1-1.5b\", trust_remote_code=True)\n",
        "# In case you want to reduce the maximum length:\n",
        "emb_model.max_seq_length = 4096"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPwggQfUS5yl"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/rag-experiment/data/2025.txt\", \"r\") as f:\n",
        "  raw_txt = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxzKF6L2THIw"
      },
      "outputs": [],
      "source": [
        "# ドキュメントを用意する。\n",
        "documents = [text.strip() for text in raw_txt.split(\"。\")]\n",
        "print(\"ドキュメントサイズ: \", len(documents))\n",
        "print(\"ドキュメントの例: \\n\", documents[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK4cYURzTHIx"
      },
      "outputs": [],
      "source": [
        "# Retrievalの実行\n",
        "question = question_df.iloc[0][\"Question\"]\n",
        "\n",
        "query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "# 各ドキュメントの類似度スコア\n",
        "scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "print(scores.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_v8gx_tTHIx"
      },
      "outputs": [],
      "source": [
        "topk = 5\n",
        "for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n",
        "  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n",
        "  print(documents[index], \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow0wZy6ETHIx"
      },
      "outputs": [],
      "source": [
        "references = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n",
        "query =  f\"[参考資料]\\n{references}\\n\\n[質問] {question_df.iloc[0]['Question']}\"\n",
        "response = generate_output(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_responses = []\n",
        "queries = []\n",
        "for index, row in question_df.iterrows():\n",
        "    question = row['Question']\n",
        "    query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "    document_embeddings = emb_model.encode(documents)\n",
        "    scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "    topk = 5\n",
        "    for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n",
        "      print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n",
        "      print(documents[index], \"\\n\\n\")\n",
        "    references = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n",
        "    query =  f\"[参考資料]\\n{references}\\n\\n[質問] {question}\"\n",
        "    queries.append(query)\n",
        "    response = generate_output(query)\n",
        "    rag_responses.append(response)\n",
        "\n",
        "result_df['Query'] = queries\n",
        "result_df['withRAG'] = rag_responses\n",
        "result_df.head()"
      ],
      "metadata": {
        "id": "0jifzeVo8kVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result_dfをcsvとして保存\n",
        "result_df.to_csv(\"/content/rag-experiment/result.csv\", index=False)"
      ],
      "metadata": {
        "id": "ypLvfQtl9w3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 評価\n",
        "# score = evaluate_answer_accuracy(question, response, gold_answer)\n",
        "# print(score)"
      ],
      "metadata": {
        "id": "XcXCKsyyHWxZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}